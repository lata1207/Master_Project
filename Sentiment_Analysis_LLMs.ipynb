{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lata1207/Master_Project/blob/main/Sentiment_Analysis_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aikHm01gP97K",
        "outputId": "7aa90f4e-54ba-4cde-c464-63eb3e3e1ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Collecting fsspec==2025.3.2\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 3.6.0\n",
            "    Uninstalling datasets-3.6.0:\n",
            "      Successfully uninstalled datasets-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.14.4 fsspec-2025.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install datasets fsspec==2025.3.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642,
          "referenced_widgets": [
            "d7391562d4c3472693dc96c0a34af4ad",
            "f1848aa0879740ac8a052f46ef212259",
            "567c13dcba444406a1cdfe77b68445ce",
            "ac8e608d6f78450093a1a184679b29b6",
            "197539ff5aa54942850008b68cec1970",
            "64a85a7d2a56469f9bdf22fb96d9d7cb",
            "4b667b07794d4ba5abb3fdc27a10d67f",
            "291cb313004a4b3994e0e1109f3c05bb",
            "58eca2abff504122a4db6622b2a0f86f",
            "aac6e91bf4594e2584b38b5e3b878f0b",
            "aca7c74852494f2dbd042ab474eff71a",
            "0b3942fe59f64e11b6ac407f3e05a5cd",
            "d94ecc33b7ba4fd19e912259b293a284",
            "7b25c6536179489a9c5a9623d1ddc363",
            "ad8f7eff46e6439ab3ff63988f78da5e",
            "1edc7bce44334ecea23fad02edeb90cb",
            "78428ea632ed46afb9045db103374f7f",
            "2e111d69842942c19c8e7f39a9069129",
            "7f8b7cb317f94bc59951a30c707f1715",
            "bd0af0780b844547bf61441795738819",
            "8aa9f876e83e4f01917d35a3ef5bd199",
            "e21c535eafdc4328ac7ae853d32fb003",
            "c964044a583d4606b86e8ef3714198d9",
            "0ed465c6732a41339195a4f373d34878",
            "7413d7883f9f4a95baef670f633723a4",
            "8ea63d13e1624252b614a92ea358f442",
            "feaf3a9d81bc437ebe258809c7b1417a",
            "ca58178a251e4a56962028131fee633a",
            "54793ef8f81b4648a3faa464c2c0ebdf",
            "6e84a63e482a4bbd9f86bfceeaf4b481",
            "c04b7278d90a4ea1bc4ccbcb033b7580",
            "f15a7fbac13a4710a63308070f532673",
            "7be8c2eeb2c84d5da7353bd671f0cb3e",
            "36bdad48dfd248748d61eedc0c17aac8",
            "44bea604c6d3468f8462d39f47c859ce",
            "69a2cc4b8ae94a3fa4373c753a57c9d7",
            "c6e87db81d0b42ba99101d6510e13d1d",
            "ab66df34f54a48d28cb9df1d74056c0b",
            "9caff1cd13444763960356454ef8947a",
            "f75fc2e14c2a477999325fb43950cd05",
            "372ce5e440734a3c80e0bfa90a304bec",
            "1f920297efc64a8b814bf051d72d0eeb",
            "3c0dca23109f4e2d9ecbc8b3b32baf42",
            "15cb8a69f5da45ab9d40eb8b964348b6",
            "dee22feaf1f34b51b8121fdac0377cc6",
            "48548fb63ddd4d609ebfc4406b3fc351",
            "aaa8b3bf6bef42928363243e5f2a3699",
            "e0e948d65a3e4209a951f80bc7e4677a",
            "36d200c20cf34cf29a750fcad3b8aa4b",
            "3ff5915cdc8440a091a898b26c62357d",
            "7b1306a7967d41029b1ced265f52d7c0",
            "2ff31e3818ab47cebe3c684dc280e763",
            "ab3de657f8df46b3b916ba0b65bf2c56",
            "4d70b375494849a0813c98151a94139c",
            "22ab9727a32a4b0cbe1e3085f597f5b9",
            "5e943607563f4833a13fe51600ccd36b",
            "f3b71e6a958b4226a3593ba875a86f3f",
            "a7df268d02e74358b74cbecbe9eb65b3",
            "b856f5ed7a99427bb37e1b02649fe06d",
            "f88b6fecac804b9187019cbe841ffbc0",
            "12a180b5f563435b9c02e34138387d40",
            "eb69f8bc413a456c90ff5784112c8107",
            "8bba9b7d929446239a1471de710da5ff",
            "a55bffe9267b4b468a068ec84a0ca6dc",
            "b5f07ad4509e45f0bd12c2428ba0eaf7",
            "561c98328923495a814e043c9df14d7b",
            "ea157138b70247d5abc8bca2015e8639",
            "b3420629650b4489be1dd6ab8e1c510e",
            "08fe51b48c704c8abdbf1671ae787010",
            "9c3c549d5aa24597bbc5f6b8d8e6bc3a",
            "508e5ff1ed02450eaf92e27d096096b4",
            "aa1c7113b4304ea7bff3175363959769",
            "34908da6f5fa4c17a2b20b5dcbbf31e9",
            "51c146f99dc64cdab27d0de6ad7ffc10",
            "12202a9ed32c458f9d3dd9f6719a5bb7",
            "842d80a94bb64a589259eff0b352af04",
            "19352877561147a5b27ee87b2acd68dd"
          ]
        },
        "id": "6Id8UGM2QlrD",
        "outputId": "6803a4df-1d00-48e5-f18d-93d65d0d4b85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7391562d4c3472693dc96c0a34af4ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b3942fe59f64e11b6ac407f3e05a5cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c964044a583d4606b86e8ef3714198d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "valid.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36bdad48dfd248748d61eedc0c17aac8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dee22feaf1f34b51b8121fdac0377cc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e943607563f4833a13fe51600ccd36b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea157138b70247d5abc8bca2015e8639",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['context', 'response'],\n",
            "        num_rows: 60005\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['context', 'response'],\n",
            "        num_rows: 6594\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['context', 'response'],\n",
            "        num_rows: 6955\n",
            "    })\n",
            "})\n",
            "{'context': 'Waiter ! ', 'response': \" I'll be with you in a second . Uh ... Yes , ma'am ? \"}\n"
          ]
        }
      ],
      "source": [
        " from datasets import load_dataset\n",
        "\n",
        "# Load the verified working dataset\n",
        "dataset = load_dataset(\"frankdarkluo/dailydialog\")\n",
        "\n",
        "# Preview structure\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499,
          "referenced_widgets": [
            "5ef10c2b7e2d49ff9e874653a0d7fffe",
            "564e3b27faea4e8a9258235715563cae",
            "378c8090f201496b88256e915a2a8e59",
            "ecaee1f458c441b180f1a9393440ea75",
            "168176d89e3b4896bdc7a780d5e16807",
            "75b0cfabfca94f81ad3d6b0b3591e06b",
            "97a47a9641e34ffab038e58920c4feba",
            "2909b4a99cd141d09d03f99d7b533e60",
            "9114e6e322e44bdeb99afc74767d8abc",
            "e2c6dc6f146a45b6b1f0b09ffddf4382",
            "31786a76e2ab4337b62d1d2ff0a6269a",
            "4b5633cca02249799f97936bad4527ab",
            "d7b149b6339f4aa693cd8612c4847676",
            "e4cf2e7b525a434f9d3ee6e5017a80d7",
            "07a253fe44c94c5eb9ba8a760243ba16",
            "3ea2c58da42648368155938ef536c750",
            "606a0a06e9254a28a5096e954848945c",
            "0b6f343df5624c2c87e19e5f9512ee5f",
            "79220a0281dc4a3abe12ddb21bf518bd",
            "a07c330d53634d34bd8faf862dda125c",
            "b5a80c6ab53f45bd83cfd23c4344a8fd",
            "02fbcfd74aac47c3afec0518b078d4a9",
            "02c6c017570740aba49d53d50b8b53c8",
            "26b02c9d97a1485f80ee147c8b6a7ea1",
            "4dffbd0cc60941b0ae5c6b0eeef204c0",
            "2b67f8e8de2049f0ba973e1cee1030c3",
            "4f9e3529393c4473a03639c51686c1ed",
            "25baf9bc0b4445eab673d18f4c3fd133",
            "f4568e5d1f03421185d343355c703500",
            "5301bd0ec0f449fbae466055d7f9b087",
            "86251085e05a4b138141e149470bac00",
            "6d00e525b4f84c8f816ab62d57b5f234",
            "5e652ee62dd342ea95093fd1f7ca1f45",
            "cb166337c5db4727b7ccfbffb5f60723",
            "bc2d99e807384da7bdaa58025ecd7b83",
            "91146e46068e402987f5d05f70fe8412",
            "46dd5fe9214f4a899a634f9f4dd7bf52",
            "300ab3660b24477ba14604bfe3e585f5",
            "a1d570131f8b49bfa6c10c30d3dba056",
            "d830daeaee124b8bac201faaad0ae1b0",
            "3e6bf3044d974af48ec48421b285ec0b",
            "f4f4b59666ec4e929d89a3ed360a38b9",
            "ed5d34fa440749fca1942ebb8e078db1",
            "3de338be48774f36bfbb14220476f9c4",
            "c32a7ec8ae844322bbdc658c6875a227",
            "fe58e0a5fb95453a946e7358808d2256",
            "245fc940369a43228323fa0d74c80552",
            "8aba02cf4b1c41e0ae5da345eee3c0e1",
            "60eb9938fde8423dbf31eeede7a8a7c7",
            "eec75434b02c4050ae84f4bde6dad342",
            "7bae57950fc540989fe1c522058245dd",
            "b403757d5c6044fab2c61d4d63491f14",
            "965d996a361b4fc19f5c59115adecbb0",
            "4af03a44a4054f538325c46f8a347ea8",
            "24df7bc74b854b288786cea1ba59e08f",
            "3040dfd35a314d8184d024ae9c183d7d",
            "cf1f3bd77bb64cb19ccfb899e7494ae5",
            "1b1ffe06c2754cd0acdaa2d0ab026c7a",
            "29372c75a4cc4fb7b3c52c204951edb0",
            "89582225227844f098a4948e296d08d2",
            "576b4d2fea994aa3a0fca13466cd706f",
            "96c9ae946d384ce4ba91b8112a19a5f7",
            "2372ee4ccdbc47bd8288c9200b737ac3",
            "fe61cc07aeb64060a4e1fbbf54bdf47d",
            "31152a68e6b242ca9cc51fff72c9a575",
            "a431ee2221d940b2a13365bdf069e102"
          ]
        },
        "id": "NQEcgvvsTN-J",
        "outputId": "9411264f-c7e2-4b15-e01f-b09daafd29b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ef10c2b7e2d49ff9e874653a0d7fffe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b5633cca02249799f97936bad4527ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02c6c017570740aba49d53d50b8b53c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb166337c5db4727b7ccfbffb5f60723",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c32a7ec8ae844322bbdc658c6875a227",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3040dfd35a314d8184d024ae9c183d7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Predicted Sentiment: LABEL_1 (score: 0.78)\n",
            "---\n",
            "Response:  This is not what I asked for . I'm afraid . \n",
            "Predicted Sentiment: LABEL_0 (score: 0.92)\n",
            "---\n",
            "Response:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Predicted Sentiment: LABEL_0 (score: 0.72)\n",
            "---\n",
            "Response:  Yes . What I ordered is roast beef , not roast beef sandwiches . \n",
            "Predicted Sentiment: LABEL_1 (score: 0.68)\n",
            "---\n",
            "Response:  Mind your own business . \n",
            "Predicted Sentiment: LABEL_1 (score: 0.59)\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Auto-Label Responses with Sentiment\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained Twitter sentiment model\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "# Annotate the first 5 responses\n",
        "for i in range(5):\n",
        "    text = dataset[\"train\"][i][\"response\"]\n",
        "    result = classifier(text)[0]\n",
        "    print(f\"Response: {text}\")\n",
        "    print(f\"Predicted Sentiment: {result['label']} (score: {result['score']:.2f})\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk-479dMYUIl",
        "outputId": "5fa9a329-1ab4-408c-bd81-a538381a97a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Predicted Sentiment: Neutral (score: 0.78)\n",
            "---\n",
            "Response:  This is not what I asked for . I'm afraid . \n",
            "Predicted Sentiment: Negative (score: 0.92)\n",
            "---\n",
            "Response:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Predicted Sentiment: Negative (score: 0.72)\n",
            "---\n",
            "Response:  Yes . What I ordered is roast beef , not roast beef sandwiches . \n",
            "Predicted Sentiment: Neutral (score: 0.68)\n",
            "---\n",
            "Response:  Mind your own business . \n",
            "Predicted Sentiment: Neutral (score: 0.59)\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "label_map = {\n",
        "    \"LABEL_0\": \"Negative\",\n",
        "    \"LABEL_1\": \"Neutral\",\n",
        "    \"LABEL_2\": \"Positive\"\n",
        "}\n",
        "\n",
        "for i in range(5):\n",
        "    text = dataset[\"train\"][i][\"response\"]\n",
        "    result = classifier(text)[0]\n",
        "    sentiment = label_map[result['label']]\n",
        "    print(f\"Response: {text}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment} (score: {result['score']:.2f})\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15u_cmeha0Z_",
        "outputId": "c5866871-c172-4e92-f69d-4cda7b2a174e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Input: Waiter !  [SEP]  I'll be with you in a second . Uh ... Yes , ma'am ? \n"
          ]
        }
      ],
      "source": [
        "# Pick a sample from the dataset\n",
        "sample = dataset[\"train\"][0]\n",
        "\n",
        "# Extract context and response\n",
        "context = sample[\"context\"]\n",
        "response = sample[\"response\"]\n",
        "\n",
        "# Combine them into one input string\n",
        "combined_input = f\"{context} [SEP] {response}\"\n",
        "\n",
        "print(\"Combined Input:\", combined_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvMW3LwZbAKn",
        "outputId": "bfbea1a1-d63c-46cf-94b0-6fab1a7a3fb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: Waiter ! \n",
            "Response:  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Combined: Waiter !  [SEP]  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Predicted Sentiment (with context): Neutral (score: 0.83)\n",
            "---\n",
            "Context:  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Response:  This is not what I asked for . I'm afraid . \n",
            "Combined:  I'll be with you in a second . Uh ... Yes , ma'am ?  [SEP]  This is not what I asked for . I'm afraid . \n",
            "Predicted Sentiment (with context): Negative (score: 0.82)\n",
            "---\n",
            "Context:  This is not what I asked for . I'm afraid . \n",
            "Response:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Combined:  This is not what I asked for . I'm afraid .  [SEP]  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Predicted Sentiment (with context): Negative (score: 0.92)\n",
            "---\n",
            "Context:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Response:  Yes . What I ordered is roast beef , not roast beef sandwiches . \n",
            "Combined:  Oh , I'm so sorry . May I ask what you ordered again ?  [SEP]  Yes . What I ordered is roast beef , not roast beef sandwiches . \n",
            "Predicted Sentiment (with context): Negative (score: 0.63)\n",
            "---\n",
            "Context: Why are you always staring at Melissa ? Do you like her or something ? \n",
            "Response:  Mind your own business . \n",
            "Combined: Why are you always staring at Melissa ? Do you like her or something ?  [SEP]  Mind your own business . \n",
            "Predicted Sentiment (with context): Negative (score: 0.57)\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Load sentiment analysis pipeline\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "# Define label mapping\n",
        "label_map = {\n",
        "    \"LABEL_0\": \"Negative\",\n",
        "    \"LABEL_1\": \"Neutral\",\n",
        "    \"LABEL_2\": \"Positive\"\n",
        "}\n",
        "\n",
        "# Test on first 5 samples using context + response\n",
        "for i in range(5):\n",
        "    sample = dataset[\"train\"][i]\n",
        "    context = sample[\"context\"]\n",
        "    response = sample[\"response\"]\n",
        "\n",
        "    combined_input = f\"{context} [SEP] {response}\"\n",
        "\n",
        "    result = classifier(combined_input)[0]\n",
        "    sentiment = label_map[result[\"label\"]]\n",
        "\n",
        "    print(f\"Context: {context}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Combined: {combined_input}\")\n",
        "    print(f\"Predicted Sentiment (with context): {sentiment} (score: {result['score']:.2f})\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeJMGdOZs3Ot",
        "outputId": "f2a2e4e8-7121-440b-d554-066d2b5b6c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sample 3 ---\n",
            "Context:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Response:  Yes . What I ordered is roast beef , not roast beef sandwiches . \n",
            "Without Context: Neutral\n",
            "With Context:    Negative\n",
            "\n",
            "--- Sample 4 ---\n",
            "Context: Why are you always staring at Melissa ? Do you like her or something ? \n",
            "Response:  Mind your own business . \n",
            "Without Context: Neutral\n",
            "With Context:    Negative\n",
            "\n",
            "--- Sample 6 ---\n",
            "Context:  A month . Don't forget to return them by the due date . \n",
            "Response:  What if I can't finish them by then ? \n",
            "Without Context: Negative\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 13 ---\n",
            "Context:  yes , we can look at the computer and computer menu at the same time . And this will help us understand both better . \n",
            "Response:  What should I learn first ? \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 17 ---\n",
            "Context:  Since summer is coming , I think swimming is a good way for you to do . \n",
            "Response:  Are you sure ? \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 19 ---\n",
            "Context:  Of course ! Swimming can help you stay in shape by targeting all parts of your body . \n",
            "Response:  Really ? Does swimming have other advantages ? \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 29 ---\n",
            "Context:  Exactly , it seems like it's going to be loads of fun . \n",
            "Response:  When exactly does the party start ? \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 37 ---\n",
            "Context:  He is surely a hard nut , nobody likes approaching him . \n",
            "Response:  We'd better leave him alone . \n",
            "Without Context: Neutral\n",
            "With Context:    Negative\n",
            "\n",
            "--- Sample 38 ---\n",
            "Context: Excuse me . I wonder if you can help me . \n",
            "Response:  I'll try my best . \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 39 ---\n",
            "Context:  I'll try my best . \n",
            "Response:  I'm completely lost . I'm trying to find the way to my daughter's home . \n",
            "Without Context: Negative\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 40 ---\n",
            "Context:  I'm completely lost . I'm trying to find the way to my daughter's home . \n",
            "Response:  Please tell me where your daughter's home is . \n",
            "Without Context: Neutral\n",
            "With Context:    Negative\n",
            "\n",
            "--- Sample 45 ---\n",
            "Context: Ok , let ’ s go through this one more time . I don ’ t want anymore ruined or dyed blouses ! \n",
            "Response:  I know , I know . OK , so I have to separate the colors from the whites and put them in this strange looking contraption so called washing machine . \n",
            "Without Context: Neutral\n",
            "With Context:    Negative\n",
            "\n",
            "--- Sample 47 ---\n",
            "Context:  Right . You have to turn it on and program it depending on what type of clothes you are washing . For example for delicates , you should set a shorter washing cycle . Also , be sure to use fabric softener and this detergent when washing . \n",
            "Response:  So complicated ! Ok , what about this red wine stain ? How do I get it out ? \n",
            "Without Context: Negative\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 49 ---\n",
            "Context:  Since this is a white t-shirt , you can just pour a little bit of bleach on it and it will do the trick . \n",
            "Response:  Cool . Then I can just throw everything in the dryer for an hour and it ’ s all set right ? \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 52 ---\n",
            "Context: Will it be all right to visit you this evening ? \n",
            "Response:  I'm sorry , but I have an appointment this evening.How about tomorrow evening ? \n",
            "Without Context: Negative\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 53 ---\n",
            "Context:  I'm sorry , but I have an appointment this evening.How about tomorrow evening ? \n",
            "Response:  It's fine with me . \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 55 ---\n",
            "Context:  Well done . \n",
            "Response:  And I've filed the letters . \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 57 ---\n",
            "Context:  Excellent job . \n",
            "Response:  And I've checked the orders . \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 59 ---\n",
            "Context:  I'm very impressed . \n",
            "Response:  And I've ordered the supplies . \n",
            "Without Context: Neutral\n",
            "With Context:    Positive\n",
            "\n",
            "--- Sample 61 ---\n",
            "Context: Did you see who the suspect was ? \n",
            "Response:  I got a perfect look at the suspect . \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 73 ---\n",
            "Context:  I see . When are you travelling ? \n",
            "Response:  I ’ d like to fly next Friday . \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 85 ---\n",
            "Context:  haven't you heard ? Brown is the new black . \n",
            "Response:  why don't you just get black ? Black suits are always fashionable and can be worn for anything — a funeral , a wedding , a job interview — anything ! \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n",
            "--- Sample 89 ---\n",
            "Context:  I like the one on the left , but I don't like the pattern on it . It's too much . I want a pattern that's a bit subtler . \n",
            "Response:  how about this one ? \n",
            "Without Context: Neutral\n",
            "With Context:    Negative\n",
            "\n",
            "--- Sample 91 ---\n",
            "Context:  I think that will do . Let's go talk to the tailor about getting it made . \n",
            "Response:  ok , let's go . \n",
            "Without Context: Positive\n",
            "With Context:    Neutral\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare performance — with vs without context\n",
        "contextual_labels = []\n",
        "non_contextual_labels = []\n",
        "\n",
        "for i in range(100):  # Use 100 samples for speed\n",
        "    sample = dataset[\"train\"][i]\n",
        "    context = sample[\"context\"]\n",
        "    response = sample[\"response\"]\n",
        "\n",
        "    # Without context\n",
        "    no_context_result = classifier(response)[0]\n",
        "    no_context_sentiment = label_map[no_context_result[\"label\"]]\n",
        "    non_contextual_labels.append(no_context_sentiment)\n",
        "\n",
        "    # With context\n",
        "    combined_input = f\"{context} [SEP] {response}\"\n",
        "    context_result = classifier(combined_input)[0]\n",
        "    context_sentiment = label_map[context_result[\"label\"]]\n",
        "    contextual_labels.append(context_sentiment)\n",
        "\n",
        "    if no_context_sentiment != context_sentiment:\n",
        "        print(f\"--- Sample {i} ---\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"Without Context: {no_context_sentiment}\")\n",
        "        print(f\"With Context:    {context_sentiment}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Ac4w7Ys4tX",
        "outputId": "567e1747-1c30-41d3-a759-bbea89a94142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sentiment changed in 24/100 samples (24%) due to context.\n"
          ]
        }
      ],
      "source": [
        "# Compute statistics (optional)\n",
        "# count how often sentiment changes due to context\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "diff_count = sum([c != nc for c, nc in zip(contextual_labels, non_contextual_labels)])\n",
        "print(f\"\\nSentiment changed in {diff_count}/100 samples ({diff_count}%) due to context.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "3649e5084f4d46449662d18f8098e5f9",
            "7f6ef122362847338823c739ee852d32",
            "4a8bf520f7454b4a8b018d0575dd1fc0",
            "ff40a0fcbd294932a2c7609a2c9070e2",
            "955e47d37bf145a8a09659a3dd978dbf",
            "10f3d6a6a9714b77bb679c7acf5fa83f",
            "4bdc2a8295114014a8104e871141a82a",
            "1c9ce367324b421da9fb6f3ff293f1a2",
            "bedf9ab403cf40ee867d4b5157c871c7",
            "981f5c8b8edc465c9b77d3c3c786c7ab",
            "a84fd5ae63b64a2990586e9712ac0465",
            "617ba50c2db946cb8c8117ebf5ce2ef3",
            "ea01a2f30b794da79f11944d558a34e1",
            "f75dfcdfaf474845a0a57520aa60c6bd",
            "a8a683bf991e4ff7923cf78cf0d88179",
            "ffed15abe46d47be98a372c04d06ccad",
            "31e1c2e64f19487ab1eb57f307af9771",
            "33d0abb14ada4900ab5378a3f4db7531",
            "fb081369d4844cce849413f3891108d8",
            "b08fde8ddd3d4c4da21c427fc9901dfd",
            "2aa30f4b9b614e5d84b5ea97d632f3ae",
            "b08ada5dfd9e4ba0aa518597efc8aaf3",
            "4cf5bf3785474af09dfbca7c3c352578",
            "6a2e1f577e024f8aa142bd3250fbc1f4",
            "59ffbe38eac4469a869e09590b9953ed",
            "e0e806b49d2a4e668fb0e6f5f6325673",
            "4b19e09cd3e9474a8df9189da643c474",
            "998eeafc695b4c57b328dc35e40b2779",
            "aaad62fcad9e4ea8a25a59f3f3a5e85d",
            "a9a2ce8d1c8e44e3a5f43388f3ff1739",
            "984f60db53a44345813d5e2bbc250f80",
            "d0290cadd58b48a8a91147c76de8935b",
            "2c6f1d68bfab48f98c11f17b0349200c",
            "0df38266777b4351b815a4dfb7bc4bd6",
            "97b2414f04904041a50d291b79a45ce6",
            "b13623f5e5a442f5abe998dbdeb321a5",
            "47c1f82840244a91abee7c9780e4e372",
            "88e2a00ae82248d79c73fcecad2cd354",
            "fc21b4977d154b598df3ae4b9a56ac19",
            "f504425ae6034dc8bbc976968bc0cc18",
            "65f6b903bc4d4b16918f7262194112b9",
            "7247d3c1037d4ea1910c079acb135482",
            "db750887d9794696a63d4bb958f9d71d",
            "8c11c048f59f44039629a55c11d72456"
          ]
        },
        "id": "s1yEJLXYutg3",
        "outputId": "0b7bf10d-88dd-40de-b650-65a8790bb6e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3649e5084f4d46449662d18f8098e5f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "617ba50c2db946cb8c8117ebf5ce2ef3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cf5bf3785474af09dfbca7c3c352578",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df38266777b4351b815a4dfb7bc4bd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1\n",
            "Context: Waiter ! \n",
            "Response:  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Sentiment (Response Only): Positive (score: 1.00)\n",
            "Sentiment (With Context): Positive (score: 1.00)\n",
            "---\n",
            "Sample 2\n",
            "Context:  I'll be with you in a second . Uh ... Yes , ma'am ? \n",
            "Response:  This is not what I asked for . I'm afraid . \n",
            "Sentiment (Response Only): Negative (score: 1.00)\n",
            "Sentiment (With Context): Negative (score: 1.00)\n",
            "---\n",
            "Sample 3\n",
            "Context:  This is not what I asked for . I'm afraid . \n",
            "Response:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Sentiment (Response Only): Negative (score: 1.00)\n",
            "Sentiment (With Context): Negative (score: 1.00)\n",
            "---\n",
            "Sample 4\n",
            "Context:  Oh , I'm so sorry . May I ask what you ordered again ? \n",
            "Response:  Yes . What I ordered is roast beef , not roast beef sandwiches . \n",
            "Sentiment (Response Only): Negative (score: 0.71)\n",
            "Sentiment (With Context): Negative (score: 1.00)\n",
            "---\n",
            "Sample 5\n",
            "Context: Why are you always staring at Melissa ? Do you like her or something ? \n",
            "Response:  Mind your own business . \n",
            "Sentiment (Response Only): Positive (score: 0.97)\n",
            "Sentiment (With Context): Positive (score: 0.82)\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Load second sentiment model (fine-tuned BERT on SST-2)\n",
        "second_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "second_classifier = pipeline(\"sentiment-analysis\", model=second_model)\n",
        "\n",
        "# Label mapping for SST-2\n",
        "label_map_sst = {\n",
        "    \"NEGATIVE\": \"Negative\",\n",
        "    \"POSITIVE\": \"Positive\"\n",
        "}\n",
        "\n",
        "# Run on first 5 samples\n",
        "for i in range(5):\n",
        "    context = dataset[\"train\"][i][\"context\"]\n",
        "    response = dataset[\"train\"][i][\"response\"]\n",
        "\n",
        "    combined_input = f\"{context} [SEP] {response}\"\n",
        "\n",
        "    result_response_only = second_classifier(response)[0]\n",
        "    result_combined = second_classifier(combined_input)[0]\n",
        "\n",
        "    sentiment_response_only = label_map_sst[result_response_only[\"label\"]]\n",
        "    sentiment_combined = label_map_sst[result_combined[\"label\"]]\n",
        "\n",
        "    print(f\"Sample {i+1}\")\n",
        "    print(f\"Context: {context}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Sentiment (Response Only): {sentiment_response_only} (score: {result_response_only['score']:.2f})\")\n",
        "    print(f\"Sentiment (With Context): {sentiment_combined} (score: {result_combined['score']:.2f})\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcDD9IYFCP97"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}