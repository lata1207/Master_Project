{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ddef89",
   "metadata": {},
   "source": [
    "\n",
    "# Context-Aware Sentiment Analysis using LLMs — End-to-End\n",
    "\n",
    "**What this notebook does**  \n",
    "- Loads **manually downloaded CSVs** (`train.csv`, `valid.csv`, `test.csv`) from `frankdarkluo/DailyDialog` (Hugging Face).  \n",
    "- Builds **context-aware inputs** by concatenating `context` + `response`.  \n",
    "- Trains & evaluates two LLM baselines:\n",
    "  - `cardiffnlp/twitter-roberta-base-sentiment` (RoBERTa, 3-class head)\n",
    "  - `distilbert-base-uncased-finetuned-sst-2-english` (DistilBERT, re-initialized head if needed)\n",
    "- Reports accuracy, macro-F1, weighted-F1, confusion matrix.  \n",
    "- Saves artifacts to `./outputs/`.\n",
    "\n",
    "> Place your CSVs in the working directory or update the paths below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8bdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # If needed, uncomment to install deps (Colab/clean venv):\n",
    "# !pip install -U transformers datasets evaluate scikit-learn matplotlib pandas numpy accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d31a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "                          EarlyStoppingCallback, pipeline, set_seed)\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "class CFG:\n",
    "    # Paths to your local CSVs\n",
    "    TRAIN_PATH = \"train.csv\"   # update if needed\n",
    "    VALID_PATH = \"valid.csv\"\n",
    "    TEST_PATH  = \"test.csv\"\n",
    "\n",
    "    # Column names (adjust if your CSVs differ)\n",
    "    CONTEXT_COL   = \"context\"\n",
    "    RESPONSE_COL  = \"response\"\n",
    "    LABEL_COLS_TRY = [\"label\", \"sentiment\", \"target\"]  # searched in this order\n",
    "    EMOTION_COLS_TRY = [\"emotion\", \"emotion_id\", \"label_emotion\"]\n",
    "\n",
    "    # Emotion -> Sentiment mapping (DailyDialog-style)\n",
    "    # If you have emotion IDs 0..6, map to names here:\n",
    "    EMOTION_ID_TO_NAME = {\n",
    "        0: \"no_emotion\",  # aka neutral\n",
    "        1: \"anger\",\n",
    "        2: \"disgust\",\n",
    "        3: \"fear\",\n",
    "        4: \"happiness\",\n",
    "        5: \"sadness\",\n",
    "        6: \"surprise\",\n",
    "    }\n",
    "    # Name -> sentiment class (3-way)\n",
    "    EMOTION_TO_SENTIMENT = {\n",
    "        \"no_emotion\": \"neutral\",\n",
    "        \"anger\": \"negative\",\n",
    "        \"disgust\": \"negative\",\n",
    "        \"fear\": \"negative\",\n",
    "        \"happiness\": \"positive\",\n",
    "        \"sadness\": \"negative\",\n",
    "        \"surprise\": \"neutral\",  # can be argued positive/neutral; choose neutral by default\n",
    "        \"neutral\": \"neutral\",\n",
    "    }\n",
    "\n",
    "    # Sentiment label set (3-class)\n",
    "    SENTIMENT_LABELS = [\"negative\", \"neutral\", \"positive\"]\n",
    "    LABEL2ID = {k:i for i,k in enumerate(SENTIMENT_LABELS)}\n",
    "    ID2LABEL = {i:k for k,i in LABEL2ID.items()}\n",
    "\n",
    "    # Models to train\n",
    "    MODELS = [\n",
    "        (\"cardiffnlp/twitter-roberta-base-sentiment\", \"roberta_twitter\"),\n",
    "        (\"distilbert-base-uncased-finetuned-sst-2-english\", \"distilbert_sst2\"),\n",
    "    ]\n",
    "\n",
    "    # Training\n",
    "    SEED = 42\n",
    "    MAX_LENGTH = 256              # truncate long dialogues\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 3\n",
    "    LR = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.06\n",
    "    PATIENCE = 2                  # early stopping on eval loss\n",
    "    LOG_STEPS = 50\n",
    "\n",
    "    # Context ablation toggle (train/eval both variants)\n",
    "    TRAIN_WITH_CONTEXT = True\n",
    "    TRAIN_WITHOUT_CONTEXT = True\n",
    "\n",
    "    # If labels are missing, auto-label using RoBERTa (silver labels)\n",
    "    AUTO_LABEL_IF_MISSING = True\n",
    "\n",
    "    # Output dir\n",
    "    OUTDIR = Path(\"outputs\")\n",
    "\n",
    "\n",
    "set_seed(CFG.SEED)\n",
    "CFG.OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _read_csv(path):\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    # Strip possible BOM or whitespace columns names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "train_df = _read_csv(CFG.TRAIN_PATH)\n",
    "valid_df = _read_csv(CFG.VALID_PATH)\n",
    "test_df  = _read_csv(CFG.TEST_PATH)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Valid shape:\", valid_df.shape)\n",
    "print(\"Test  shape:\",  test_df.shape)\n",
    "print(\"Columns (train):\", list(train_df.columns)[:20])\n",
    "\n",
    "# Ensure required text columns exist\n",
    "for col in [CFG.CONTEXT_COL, CFG.RESPONSE_COL]:\n",
    "    if col not in train_df.columns:\n",
    "        raise KeyError(f\"Expected column '{col}' not found in train.csv. Found: {train_df.columns}\")\n",
    "    if col not in valid_df.columns or col not in test_df.columns:\n",
    "        raise KeyError(f\"Expected column '{col}' missing from valid/test CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discover_or_create_labels(df: pd.DataFrame) -> pd.Series:\n",
    "    # 1) Direct sentiment column?\n",
    "    for c in CFG.LABEL_COLS_TRY:\n",
    "        if c in df.columns:\n",
    "            vals = df[c]\n",
    "            # If categorical strings like 'positive', 'neutral', 'negative'\n",
    "            if vals.dtype == object:\n",
    "                return vals.map(lambda x: str(x).strip().lower())\n",
    "            # If numeric IDs 0..N, assume already mapped to 3-class or convert via emotion if needed\n",
    "            if np.issubdtype(vals.dtype, np.integer):\n",
    "                # Heuristic: if max id <= 2, assume already 3-class sentiment\n",
    "                if int(vals.max()) <= 2:\n",
    "                    inv = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
    "                    return vals.map(inv)\n",
    "                # Otherwise treat as emotion IDs 0..6 -> map to sentiment\n",
    "                else:\n",
    "                    emo_names = vals.map(CFG.EMOTION_ID_TO_NAME)\n",
    "                    return emo_names.map(CFG.EMOTION_TO_SENTIMENT)\n",
    "\n",
    "    # 2) Emotion column -> map to sentiment\n",
    "    for c in CFG.EMOTION_COLS_TRY:\n",
    "        if c in df.columns:\n",
    "            vals = df[c]\n",
    "            if np.issubdtype(vals.dtype, np.integer):\n",
    "                emo_names = vals.map(CFG.EMOTION_ID_TO_NAME)\n",
    "            else:\n",
    "                emo_names = vals.map(lambda x: str(x).strip().lower())\n",
    "            return emo_names.map(CFG.EMOTION_TO_SENTIMENT)\n",
    "\n",
    "    # 3) No labels present — optionally auto-label via RoBERTa\n",
    "    if CFG.AUTO_LABEL_IF_MISSING:\n",
    "        print(\"No label/emotion column found — auto-labeling with RoBERTa sentiment model (silver labels).\")\n",
    "        clf = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment', top_k=None)\n",
    "        texts = (df[CFG.CONTEXT_COL].fillna('') + \" [SEP] \" + df[CFG.RESPONSE_COL].fillna('')).tolist()\n",
    "        preds = []\n",
    "        for t in texts:\n",
    "            out = clf(t)[0]  # [{'label': 'LABEL_0', 'score': ...}, ...] depending on model\n",
    "            # cardiffnlp returns labels like 'negative', 'neutral', 'positive'\n",
    "            # To be safe, pick top label by score\n",
    "            top = max(out, key=lambda x: float(x['score']))\n",
    "            preds.append(top['label'].lower())\n",
    "        return pd.Series(preds, index=df.index)\n",
    "    else:\n",
    "        raise ValueError(\"No label/emotion column found and AUTO_LABEL_IF_MISSING=False. Please add labels.\")\n",
    "\n",
    "# Build unified 'sentiment' column (str): 'negative'/'neutral'/'positive'\n",
    "train_df['sentiment'] = discover_or_create_labels(train_df)\n",
    "valid_df['sentiment'] = discover_or_create_labels(valid_df)\n",
    "test_df['sentiment']  = discover_or_create_labels(test_df)\n",
    "\n",
    "# Drop any rows with missing labels\n",
    "def _drop_missing_labels(df):\n",
    "    before = len(df)\n",
    "    df = df[df['sentiment'].isin(CFG.SENTIMENT_LABELS)].copy()\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f\"Dropped {before-after} rows due to invalid labels.\")\n",
    "    return df\n",
    "\n",
    "train_df = _drop_missing_labels(train_df)\n",
    "valid_df = _drop_missing_labels(valid_df)\n",
    "test_df  = _drop_missing_labels(test_df)\n",
    "\n",
    "# Encode labels\n",
    "train_df['label_id'] = train_df['sentiment'].map(CFG.LABEL2ID)\n",
    "valid_df['label_id'] = valid_df['sentiment'].map(CFG.LABEL2ID)\n",
    "test_df['label_id']  = test_df['sentiment'].map(CFG.LABEL2ID)\n",
    "\n",
    "print(train_df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- EDA: Class distribution -----\n",
    "def plot_class_distribution(df, title):\n",
    "    counts = df['sentiment'].value_counts().reindex(CFG.SENTIMENT_LABELS).fillna(0)\n",
    "    plt.figure()\n",
    "    counts.plot(kind='bar')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(train_df, \"Train class distribution\")\n",
    "plot_class_distribution(valid_df, \"Validation class distribution\")\n",
    "plot_class_distribution(test_df,  \"Test class distribution\")\n",
    "\n",
    "# ----- EDA: Dialogue length (chars) -----\n",
    "def plot_length_hist(df, title):\n",
    "    lens = (df[CFG.CONTEXT_COL].fillna('') + \" \" + df[CFG.RESPONSE_COL].fillna('')).str.len()\n",
    "    plt.figure()\n",
    "    plt.hist(lens, bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Combined context+response length (chars)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "plot_length_hist(train_df, \"Length distribution (train)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build two views:\n",
    "#  A) With context:   \"CTX: {context} [SEP] RESP: {response}\"\n",
    "#  B) Response-only:  \"{response}\"\n",
    "SEP = \" [SEP] \"\n",
    "\n",
    "train_df['text_with_ctx'] = \"CTX: \" + train_df[CFG.CONTEXT_COL].fillna('') + SEP + \"RESP: \" + train_df[CFG.RESPONSE_COL].fillna('')\n",
    "valid_df['text_with_ctx'] = \"CTX: \" + valid_df[CFG.CONTEXT_COL].fillna('') + SEP + \"RESP: \" + valid_df[CFG.RESPONSE_COL].fillna('')\n",
    "test_df['text_with_ctx']  = \"CTX: \" + test_df[CFG.CONTEXT_COL].fillna('')  + SEP + \"RESP: \" + test_df[CFG.RESPONSE_COL].fillna('')\n",
    "\n",
    "train_df['text_resp_only'] = train_df[CFG.RESPONSE_COL].fillna('')\n",
    "valid_df['text_resp_only'] = valid_df[CFG.RESPONSE_COL].fillna('')\n",
    "test_df['text_resp_only']  = test_df[CFG.RESPONSE_COL].fillna('')\n",
    "\n",
    "# Convert to HF Datasets\n",
    "def to_hf_dataset(df, text_col):\n",
    "    return Dataset.from_pandas(df[[text_col, 'label_id']].rename(columns={text_col:'text'}), preserve_index=False)\n",
    "\n",
    "ds_with_ctx = DatasetDict({\n",
    "    \"train\": to_hf_dataset(train_df, 'text_with_ctx'),\n",
    "    \"validation\": to_hf_dataset(valid_df, 'text_with_ctx'),\n",
    "    \"test\": to_hf_dataset(test_df, 'text_with_ctx'),\n",
    "})\n",
    "\n",
    "ds_resp_only = DatasetDict({\n",
    "    \"train\": to_hf_dataset(train_df, 'text_resp_only'),\n",
    "    \"validation\": to_hf_dataset(valid_df, 'text_resp_only'),\n",
    "    \"test\": to_hf_dataset(test_df, 'text_resp_only'),\n",
    "})\n",
    "\n",
    "print(ds_with_ctx)\n",
    "print(ds_resp_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenise_dataset(ds: DatasetDict, tokenizer):\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, max_length=CFG.MAX_LENGTH)\n",
    "    return ds.map(_tok, batched=True, remove_columns=['text'])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    _, _, f1_weighted, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": p,\n",
    "        \"recall_macro\": r,\n",
    "    }\n",
    "\n",
    "def train_and_eval(model_name: str, short_name: str, ds: DatasetDict, run_suffix: str):\n",
    "    print(f\"\\n==== Training {model_name} ({short_name}) — {run_suffix} ====\")\n",
    "    outdir = CFG.OUTDIR / f\"{short_name}_{run_suffix}\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenized = tokenise_dataset(ds, tokenizer)\n",
    "\n",
    "    num_labels = len(CFG.SENTIMENT_LABELS)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=CFG.ID2LABEL,\n",
    "        label2id=CFG.LABEL2ID,\n",
    "        ignore_mismatched_sizes=True,   # re-init head if needed\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(outdir / \"hf_runs\"),\n",
    "        per_device_train_batch_size=CFG.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=CFG.BATCH_SIZE,\n",
    "        learning_rate=CFG.LR,\n",
    "        num_train_epochs=CFG.EPOCHS,\n",
    "        weight_decay=CFG.WEIGHT_DECAY,\n",
    "        warmup_ratio=CFG.WARMUP_RATIO,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=CFG.LOG_STEPS,\n",
    "        report_to=\"none\",\n",
    "        seed=CFG.SEED,\n",
    "        dataloader_num_workers=2,\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStoppingCallback(early_stopping_patience=CFG.PATIENCE)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stop],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate\n",
    "    eval_val = trainer.evaluate(tokenized[\"validation\"])\n",
    "    eval_test = trainer.evaluate(tokenized[\"test\"])\n",
    "\n",
    "    # Predictions & confusion matrix on test\n",
    "    test_out = trainer.predict(tokenized[\"test\"])\n",
    "    test_preds = np.argmax(test_out.predictions, axis=-1)\n",
    "    cm = confusion_matrix(test_out.label_ids, test_preds, labels=list(range(num_labels)))\n",
    "\n",
    "    # Save artifacts\n",
    "    trainer.save_model(str(outdir / \"model\"))\n",
    "    tokenizer.save_pretrained(str(outdir / \"model\"))\n",
    "    np.save(outdir / \"confusion_matrix.npy\", cm)\n",
    "    with open(outdir / \"metrics_val.json\", \"w\") as f: json.dump(eval_val, f, indent=2)\n",
    "    with open(outdir / \"metrics_test.json\", \"w\") as f: json.dump(eval_test, f, indent=2)\n",
    "\n",
    "    # Plot CM\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(f\"Confusion Matrix — {short_name} ({run_suffix})\")\n",
    "    tick_marks = np.arange(num_labels)\n",
    "    plt.xticks(tick_marks, CFG.SENTIMENT_LABELS, rotation=45)\n",
    "    plt.yticks(tick_marks, CFG.SENTIMENT_LABELS)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return summary row\n",
    "    return {\n",
    "        \"model\": short_name,\n",
    "        \"run\": run_suffix,\n",
    "        \"val_accuracy\": eval_val.get(\"eval_accuracy\"),\n",
    "        \"val_f1_macro\": eval_val.get(\"eval_f1_macro\"),\n",
    "        \"val_f1_weighted\": eval_val.get(\"eval_f1_weighted\"),\n",
    "        \"test_accuracy\": eval_test.get(\"eval_accuracy\"),\n",
    "        \"test_f1_macro\": eval_test.get(\"eval_f1_macro\"),\n",
    "        \"test_f1_weighted\": eval_test.get(\"eval_f1_weighted\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "if CFG.TRAIN_WITH_CONTEXT:\n",
    "    for model_name, short_name in CFG.MODELS:\n",
    "        row = train_and_eval(model_name, short_name, ds_with_ctx, run_suffix=\"with_ctx\")\n",
    "        results.append(row)\n",
    "\n",
    "if CFG.TRAIN_WITHOUT_CONTEXT:\n",
    "    for model_name, short_name in CFG.MODELS:\n",
    "        row = train_and_eval(model_name, short_name, ds_resp_only, run_suffix=\"resp_only\")\n",
    "        results.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "res_path = CFG.OUTDIR / \"summary_results.csv\"\n",
    "res_df.to_csv(res_path, index=False)\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c275ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(results):\n",
    "    best_idx = int(np.argmax([r['val_f1_macro'] for r in results]))\n",
    "    best = results[best_idx]\n",
    "    print(\"Best (by val macro-F1):\", best)\n",
    "\n",
    "    # Reload artifacts to print a detailed report\n",
    "    run_dir = CFG.OUTDIR / f\"{best['model']}_{best['run']}\"\n",
    "    # Recreate the dataset & trainer (quick path)\n",
    "    model_name = [m for m in CFG.MODELS if m[1]==best['model']][0][0]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    ds = ds_with_ctx if best['run']==\"with_ctx\" else ds_resp_only\n",
    "    tokenized = tokenise_dataset(ds, tokenizer)\n",
    "\n",
    "    num_labels = len(CFG.SENTIMENT_LABELS)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        str(run_dir / \"model\"), num_labels=num_labels, ignore_mismatched_sizes=True\n",
    "    )\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    args = TrainingArguments(output_dir=str(run_dir/\"tmp\"), per_device_eval_batch_size=CFG.BATCH_SIZE, report_to=\"none\")\n",
    "    trainer = Trainer(model=model, args=args, tokenizer=tokenizer, data_collator=collator)\n",
    "\n",
    "    test_out = trainer.predict(tokenized[\"test\"])\n",
    "    preds = np.argmax(test_out.predictions, axis=-1)\n",
    "    print(classification_report(test_out.label_ids, preds, target_names=CFG.SENTIMENT_LABELS, digits=4))\n",
    "else:\n",
    "    print(\"No results to summarize.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
